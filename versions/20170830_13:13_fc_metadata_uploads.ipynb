{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Firecloud: Uploading Metadata to Firecloud\n",
    "Mimoun Cadosch 7/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from firecloud import api as firecloud_api\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import datetime\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## Resources\n",
    "# https://github.com/broadinstitute/fiss/blob/master/firecloud/api.py\n",
    "# https://github.com/broadinstitute/firecloud-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# res = firecloud_api.delete_workspace_config(namespace, workspace, \"tsca\", \"Mutect2TumorOnly\")\n",
    "# res = firecloud_api.delete_workspace_config(namespace, workspace, 'tsca', 'CreatePanelOfNormalsGATK')\n",
    "# delete_sample(namespace, workspace, \"AA66-Tumor-SM-F29RQ\")\n",
    "# res = delete_sample_set(namespace, workspace, sample_set_id='CumPoN_1920')\n",
    "# firecloud_api.delete_repository_method('tsca', 'CallSomaticCNV', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "namespace = \"nci-mimoun-bi-org\"\n",
    "workspace = \"CCLF_TSCA\"\n",
    "google_bucket_id = \"fc-35446f22-ea37-483a-bd6c-5e9fc56851ff\"\n",
    "path_to_all_samples_info = \"paths_to_samples_info.xlsx\"\n",
    "# batches_info = pd.read_excel('paths_to_samples_info.xlsx')\n",
    "# Must be True\n",
    "filesystem_mounted = os.path.ismount('/xchip/clf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def delete_sample(namespace, workspace, sample_id):\n",
    "    \"\"\"Delete sample from workspace/namespace\n",
    "    Args: \n",
    "        Self-explanatory\n",
    "    Returns: \n",
    "        HTTP Response\n",
    "    \"\"\"\n",
    "    body = [{\"entityType\": \"sample\", \"entityName\": sample_id}]\n",
    "    res = firecloud_api.delete_entities(namespace, workspace, body)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_sample_set(namespace, workspace, sample_set_id):\n",
    "    \"\"\"Delete sample set from workspace/namespace\n",
    "    Args: \n",
    "        Self-explanatory\n",
    "    Returns: \n",
    "        HTTP Response\n",
    "    \"\"\"\n",
    "    body = [{\"entityType\": \"sample_set\", \"entityName\": sample_set_id}]\n",
    "    res = firecloud_api.delete_entities(namespace, workspace, body)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_workspace_config(namespace, workspace, cnamespace, config):\n",
    "    \"\"\"Delete workspace configuration\n",
    "    Args: \n",
    "        Self-explanatory\n",
    "    Returns: \n",
    "        HTTP Response\n",
    "    \"\"\"\n",
    "    res = firecloud_api.delete_workspace_config(namespace, workspace, cnamespace, config)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_entity_attributes(namespace, workspace, entity_type, entity_name, attrs):\n",
    "    \"\"\"Delete entity attributes\n",
    "    Args: \n",
    "    - updates: list of attributes to delete\n",
    "    \"\"\"\n",
    "    attr_update = [{\"op\": \"RemoveAttribute\", \"attributeName\":  attr} for attr in attrs]\n",
    "    res = firecloud_api.update_entity(namespace, workspace, entity_type, entity_name, attr_update)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_entities_from_tsv(namespace, workspace, entities_tsv_file):\n",
    "    \"\"\"Upload entities from tsv file\n",
    "    Args: \n",
    "        Self-explanatory\n",
    "        entities_tsv_file: path to tsv file\n",
    "    Returns: \n",
    "        HTTP Response\n",
    "    \"\"\"\n",
    "    res = firecloud_api.upload_entities_tsv(namespace, workspace, entities_tsv=entities_tsv_file)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def patients_for_metadata_import(path, tsca_id):\n",
    "    \"\"\"Create participant entities file for Firecloud. \n",
    "    Patients need to exist before you can upload their respective samples\n",
    "    Args:\n",
    "        path_id: path to file ending in {}.import_samples.txt\n",
    "        tsca_id: tsca id\n",
    "    Saves: \n",
    "        ./tsca_id/fc_upload_patients_tsca_{tsca_id}.csv:\n",
    "            contains patient ids in tsca batch\n",
    "    \"\"\"    \n",
    "    raw = pd.read_table(path)\n",
    "    print( \"%d Participants in this batch\" % raw['individual_id'].unique().shape[0] )\n",
    "    # Data to upload\n",
    "    data = pd.DataFrame(raw.individual_id.drop_duplicates()).rename(columns={'individual_id':'entity:participant_id'})\n",
    "    os.system('mkdir -p %s'%tsca_id)\n",
    "    filename = './%s/fc_upload_patients_tsca_%s.txt' % (tsca_id, tsca_id)\n",
    "    data.to_csv(filename, '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_sample_set_for_metadata_import(path, tsca_id):\n",
    "    \"\"\"Create sample_set entities file for Firecloud.\n",
    "    A sample for a given batch \n",
    "    Args:\n",
    "        path: path to file ending in {}.import_samples.txt\n",
    "        tsca_id: batch tsca id\n",
    "    \"\"\"\n",
    "    raw = pd.read_table(path)\n",
    "    print( \"%d Samples in this batch\" % raw.shape[0] )\n",
    "\n",
    "    # Data to upload\n",
    "    data = pd.concat([pd.DataFrame(index=raw.index, columns=['membership:sample_set_id'], data=tsca_id), \\\n",
    "                      raw.sample_id], axis=1)\n",
    "    os.system('mkdir -p %s'%tsca_id)\n",
    "    filename = './%s/fc_upload_sample_set_tsca_%s.txt' % (tsca_id, tsca_id)\n",
    "    data.to_csv(filename, '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def batch_samples_for_metadata_import(path, tsca_id, google_bucket_id):\n",
    "    \"\"\"Prepare the file to import samples metadata to firecloud\n",
    "    Args:\n",
    "        path_id: path to file ending in {}.import_samples.txt\n",
    "        tsca_id: TSCAXX\n",
    "        google_bucket_id: id of google bucket ('gs://google_bucket_id')\n",
    "    Returns:\n",
    "        pd.DF of data ready for import\n",
    "    Saves:\n",
    "        ./{tsca_id}/fc_upload_samples_tsca_{tsca_id}.txt\n",
    "    \"\"\"\n",
    "    # Import raw data\n",
    "    data = pd.read_table(path)\n",
    "    \n",
    "    # Rename columns to match firecloud requirements\n",
    "    data = data.rename(columns={'sample_id':'entity:sample_id', 'individual_id':'participant_id'})\n",
    "    \n",
    "    # Locations of BAM files in google bucket\n",
    "    path_in_bucket_full = \"gs://%s/seq_data/%s\" % (google_bucket_id, tsca_id)\n",
    "\n",
    "    # Extract bam filename\n",
    "    data['bam_filename'] = data.apply(lambda row: row['clean_bam_file_capture'].split('/')[-1], axis=1)\n",
    "    \n",
    "    # Create bai filename (change extension on .bam file)\n",
    "    data['bai_filename'] = data.apply(lambda row: \"%s%s\" %(row['bam_filename'][:-3], 'bai'), axis=1)\n",
    "    \n",
    "    # Change BAM path from xchip to Google cloud\n",
    "    data['clean_bam_file_capture'] = \\\n",
    "        data.apply( lambda row: \"%s/%s/%s\" \\\n",
    "                   %(path_in_bucket_full, row['external_id_validation'], row['bam_filename']), axis=1)\n",
    "    \n",
    "    # Add location of .bai file \n",
    "    data['clean_bai_file_capture'] = \\\n",
    "        data.apply( lambda row: \"%s/%s/%s\" \\\n",
    "                   %(path_in_bucket_full, row['external_id_validation'], row['bai_filename']), axis=1)\n",
    "       \n",
    "    # Add TSCA ID\n",
    "    data['tsca_id'] = tsca_id\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def panel_of_normals_for_metadata_import(paths, N, name):\n",
    "    \"\"\"Create panel of normals sample set for Firecloud from multiple TSCA batches\n",
    "    Args:\n",
    "        paths: (list) paths to file ending in {}.import_samples.txt\n",
    "        N: (int) number of samples in panel of normals\n",
    "        name: (string) name of Panel of Normals\n",
    "    \"\"\"    \n",
    "    dfs = [ pd.read_table(paths[0]) ]\n",
    "    for i, path in enumerate(paths[1:]):\n",
    "        df_to_concat = pd.read_table(path)\n",
    "        dfs.append(df_to_concat)\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    # Shuffle samples to pick from all batches\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    normals = df[df.sample_type==\"Normal\"][:N]['sample_id']\n",
    "    if N==-1: print (\"Creating panel of %d normals\" %normals.shape[0])\n",
    "    else: print (\"Creating panel of %d normals\" %N)\n",
    "    \n",
    "    data = pd.concat([pd.DataFrame(index=normals.index, columns=['membership:sample_set_id'], data=name), \\\n",
    "                        normals], axis=1)\n",
    "\n",
    "    os.system('mkdir -p PoNs')\n",
    "    filename = './PoNs/fc_upload_PoN_sample_set_tsca_%s.txt' % (name)\n",
    "    data.to_csv(filename, '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_df_to_csv(data, tsca_id):\n",
    "    data.to_csv('%s/fc_upload_samples_tsca_%s.txt' % (tsca_id, tsca_id), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_samples(paths_to_samples_info, google_bucket_id):\n",
    "    \"\"\"Compile all samples from all batches\n",
    "    Args: Self-explanatory\n",
    "        - paths_to_samples_info: .xlsx file containing paths to files containing sample_info\n",
    "    Returns: \n",
    "        - df with samples from all batches\n",
    "    \"\"\"\n",
    "    paths_to_samples_info = pd.read_excel(paths_to_samples_info, index_col=0)\n",
    "    df_list = []\n",
    "\n",
    "    for tsca_id, paths in paths_to_samples_info.iterrows():\n",
    "        # Make data Firecloud-compatible\n",
    "        batch_data = batch_samples_for_metadata_import(paths.path_to_samples_info, tsca_id, google_bucket_id)\n",
    "        df_list.append(batch_data)\n",
    "\n",
    "    all_samples = pd.concat(df_list, axis=0)\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_matching_samples(all_samples, batch_samples):\n",
    "    \"\"\"Add sample_id and bam filepath of matching normals and primary tumor tissue for every sample\n",
    "    Args:\n",
    "        - all_samples: df with target samples we want to find matches in\n",
    "        - batch_samples: df with source samples we want to find matches for\n",
    "    Returns: \n",
    "        - batch_samples (augmented)\n",
    "    \"\"\"\n",
    "    for index, row in batch_samples.iterrows():\n",
    "        # Find all samples from same individual (same individual_id, different sample_id)\n",
    "        patient_samples = all_samples[ (all_samples['participant_id'] == row['participant_id']) \\\n",
    "                                      & (all_samples['entity:sample_id'] != row['entity:sample_id']) ]\n",
    "\n",
    "        # NOTE: If more than one match tumor tissue or match normal found, select one at random.\n",
    "        # The match normal is used to compute allelic fractions in Mutect2, so for now we ignore the conditions it was grown in.\n",
    "\n",
    "        # Tumor tissue: Add primary tumor tissue\n",
    "        match_primary_tumor = patient_samples[ patient_samples['external_id_validation'] \\\n",
    "                                              .str.contains('primary|prim|tissue|tiss') ]\n",
    "        #    > No primary tumor tissue found\n",
    "        if match_primary_tumor.empty:\n",
    "            batch_samples.loc[index, 'match_primary_tumor_sample_id'] = \"NA\"\n",
    "            batch_samples.loc[index, 'match_primary_tumor_bam_file'] = \"NA\"\n",
    "        #    > Tumor tissue found\n",
    "        elif match_primary_tumor.shape[0] > 0:\n",
    "            match_primary_tumor = match_primary_tumor.sample(n=1)\n",
    "            batch_samples.loc[index, 'match_primary_tumor_sample_id'] = match_primary_tumor['entity:sample_id'].item()\n",
    "            batch_samples.loc[index, 'match_primary_tumor_bam_file'] = match_primary_tumor['clean_bam_file_capture'].item()\n",
    "\n",
    "        # Add match normal\n",
    "        match_normal = patient_samples[ patient_samples['sample_type'] == \"Normal\"]\n",
    "        #   > No match normal found\n",
    "        if match_normal.empty: \n",
    "            batch_samples.loc[index, 'match_normal_sample_id'] = \"NA\"\n",
    "            batch_samples.loc[index, 'match_normal_bam_file'] = \"NA\"\n",
    "        #   > Match normal found\n",
    "        elif match_normal.shape[0] > 0:\n",
    "            match_normal = match_normal.sample(n=1)\n",
    "            batch_samples.loc[index, 'match_normal_sample_id'] = match_normal['entity:sample_id'].item()\n",
    "            batch_samples.loc[index, 'match_normal_bam_file'] = match_normal['clean_bam_file_capture'].item()\n",
    "            \n",
    "    return batch_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_all_metadata(tsca_id, path_to_batch_samples_info, path_to_all_samples_info):    \n",
    "    \"\"\"Prepare all metadata for uploading to Firecloud\n",
    "    Args:\n",
    "        - path_to_batch_samples_info: path to info on batch samples\n",
    "        - path_to_all_samples_info: path to info on all samples\n",
    "    \"\"\"\n",
    "    patients_for_metadata_import(path_to_batch_samples_info, tsca_id)\n",
    "    batch_sample_set_for_metadata_import(path_to_batch_samples_info, tsca_id)\n",
    "    batch_samples = batch_samples_for_metadata_import(path_to_batch_samples_info, tsca_id, google_bucket_id)\n",
    "    # Collect all samples\n",
    "    all_samples = compile_samples(path_to_all_samples_info, google_bucket_id)\n",
    "    # Add match normals and primaries\n",
    "    batch_samples_with_matches = add_matching_samples(all_samples, batch_samples)\n",
    "    write_df_to_csv(batch_samples_with_matches, tsca_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_metadata(tsca_id):\n",
    "    \"\"\"Export metadata into Firecloud\n",
    "    Args: \n",
    "        - tsca_id\n",
    "    \"\"\"\n",
    "    patient_metadata    = \"%s/fc_upload_patients_tsca_%s.txt\" % (tsca_id, tsca_id)\n",
    "    sample_set_metadata = \"%s/fc_upload_sample_set_tsca_%s.txt\" % (tsca_id, tsca_id)\n",
    "    samples_metadata    = \"%s/fc_upload_samples_tsca_%s.txt\" % (tsca_id, tsca_id)\n",
    "    pon_metadata        = \"PoNs/fc_upload_PoN_sample_set_tsca_%s_PoN.txt\" %(tsca_id)\n",
    "\n",
    "    # Upload metadata\n",
    "    r1 = upload_entities_from_tsv(namespace, workspace, patient_metadata)\n",
    "    r2 = upload_entities_from_tsv(namespace, workspace, samples_metadata)\n",
    "    r3 = upload_entities_from_tsv(namespace, workspace, sample_set_metadata)\n",
    "    # r4 = upload_entities_from_tsv(namespace, workspace, pon_metadata)\n",
    "    return (r1, r2, r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cumulative_pon(paths, num_normals, pon_name, export_to_fc=False):\n",
    "    \"\"\"Create Cumulative PoN\n",
    "    Args:\n",
    "        - paths: list of paths to batch info files\n",
    "        - tsca_ids: list of tsca_ids used in this batch\n",
    "        - export_to_fc: export to firecloud\n",
    "    \"\"\"\n",
    "    panel_of_normals_for_metadata_import(paths, num_normals, pon_name)\n",
    "    if export_to_fc:\n",
    "        return upload_entities_from_tsv(namespace, workspace, 'PoNs/fc_upload_PoN_sample_set_tsca_%s.txt'%pon_name)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_method_config_version(namespace, workspace, method_namespace, method_config_name):\n",
    "    res = firecloud_api.get_workspace_config(namespace, workspace, method_namespace, method_config_name)\n",
    "    return res.json()['methodRepoMethod']['methodVersion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_method_config_versions():\n",
    "    \"\"\"Updates the file method_configs/latest_method_configs.txt\n",
    "    This file contains the latest snapshot versions of the methods used in Firecloud\n",
    "    \"\"\"\n",
    "    method_configs = pd.read_table('method_configs/latest_method_configs.txt')\n",
    "    method_configs['snapshot'] = method_configs['method'].apply(lambda x: get_method_config_version(namespace, workspace, 'tsca', x))\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H:%M\")\n",
    "    method_configs.to_csv('method_configs/%s_method_configs.txt'%timestamp, index=False, sep=\"\\t\")\n",
    "    method_configs.to_csv('method_configs/latest_method_configs.txt', index=False, sep=\"\\t\")\n",
    "    return method_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_wdls():\n",
    "    \"\"\"Update WDL scripts in wdls/production directory, to the ones currently being used in Firecloud\n",
    "    \"\"\"\n",
    "    update_method_config_versions()\n",
    "    method_configs = pd.read_table('method_configs/latest_method_configs.txt')\n",
    "    for idx, method in method_configs.iterrows():\n",
    "        res = firecloud_api.get_repository_method('tsca', method.method, method.snapshot)\n",
    "        print(\"Updating WDL for %s:%s\"%(method.method, method.snapshot))\n",
    "        if res.status_code == 200:\n",
    "            text_file = open(\"../wdls/production/%s.wdl\"%method.method, \"w\")\n",
    "            text_file.write(res.json()['payload'])\n",
    "            text_file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attrs = [\"cnv_calls_reduced_segment_mean_img\", \"cnv_calls_reduced_segment_mean_raw\"]\n",
    "# res = delete_entity_attributes(namespace, workspace, \"sample_set\", \"TSCA19\", attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_batch_metadata(tsca_id, path_to_batch_samples_info, path_to_all_samples_info):\n",
    "    \"\"\"Upload to Firecloud all the metadata necessary to run TSCA pipeline on new batch\n",
    "    Args: \n",
    "        - tsca_id: tsca_id of the batch to run TSCA pipeline on\n",
    "        - path_to_batch_samples_info: ends in *.import_samples.txt\n",
    "        - path_to_all_samples_info: .xlsx file with path_to_batch_samples_info for all batches\n",
    "    \"\"\"\n",
    "    # Prepare all metadata for batch\n",
    "    prepare_all_metadata(tsca_id, path_to_batch_samples_info, path_to_all_samples_info)\n",
    "    # Upload to Firecloud\n",
    "    export_metadata(batch_id)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run for every new batch.\n",
    "    It is also necessary to upload metadata for all previous batches, because this \n",
    "    \"\"\"\n",
    "    print (\"For new batch: please add batch_id and path_to_batch_info to paths_to_samples_info.xlsx file\")\n",
    "    s = input('Have you updated the paths_to_samples_info.xlsx file? (Y/N)')\n",
    "    if s == \"N\":\n",
    "        print(\"Please do so before proceeding...\")\n",
    "        return\n",
    "    path_to_all_samples_info = \"paths_to_samples_info.xlsx\"\n",
    "    batches_info = pd.read_excel(path_to_all_samples_info)\n",
    "#     for idx, batch in batches_info.iterrows():\n",
    "#         update_batch_metadata(batch.tscaid, batch.path_to_samples_info, path_to_all_samples_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please add batch_id and path_to_batch_info to paths_to_samples_info.xlsx file\n",
      "Have you updated the paths_to_samples_info.xlsx file? (Y/N)N\n"
     ]
    }
   ],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### Prepare all metadata for batch\n",
    "# prepare_all_metadata('TSCA20', '/xchip/clf/seq_data/processed_for_fh/tsca20_201707_SN0125362/tsca20_201707_SN0125362.import_samples.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### Create PoN for all batches\n",
    "# for index, value in paths_to_samples_info.iterrows():\n",
    "#     paths = [value.path_to_samples_info]\n",
    "#     pon_id = \"%s_PoN\" % value.tsca_id\n",
    "#     panel_of_normals_for_metadata_import(paths, -1, pon_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Export metadata for batch\n",
    "# r1, r2, r3 = export_metadata('TSCA14')\n",
    "# for index, value in paths_to_samples_info.iterrows():\n",
    "#     export_metadata(value.tsca_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### Delete sample sets\n",
    "# for tsca_id in paths_to_samples_info['tsca_id'].tolist():\n",
    "#     delete_sample_set(namespace, workspace, \"%s_PoN\" %tsca_id)\n",
    "# delete_sample_set(namespace, workspace, \"CumPon40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Deleting a sample\n",
    "# res = delete_sample_set(namespace, workspace, \"TSCA14\")\n",
    "# res = delete_sample(namespace, workspace, 'DW039-Tumor-SM-DB2IF')\n",
    "### NOTE: Delete sample (manually) from samples fc_upload file AND from sample_set membership fc_upload file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create cumulative PoN\n",
    "# res = create_cumulative_pon(batches_info.path_to_samples_info.tolist(), 5, 'CumPon5', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_samples = compile_samples('paths_to_samples_info.xlsx', google_bucket_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pon = pd.read_table('PoNs/fc_upload_PoN_sample_set_tsca_CumPon40.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pon.merge(all_samples, left_on='sample_id', right_on='entity:sample_id')[['sample_id', 'tsca_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update_method_config_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_remote_samples_metadata(filename):\n",
    "    \"\"\"Download metedata of all samples to filename\n",
    "    Args:\n",
    "        - filename: file to write samples metadata to\n",
    "    \"\"\"\n",
    "    res = firecloud_api.get_entities_tsv(namespace, workspace, \"sample\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in res.iter_content(chunk_size=1024): \n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_samples_with_oncotated_mafs(filename):\n",
    "    remote_samples = pd.read_table(filename)\n",
    "    #udpated_local_samples = pd.merge(local_samples, remote_samples[['entity:sample_id', 'oncotated_maf']], on='entity:sample_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"match_normal_updates/remote_samples.tsv\"\n",
    "download_remote_samples(filename)\n",
    "update_samples_with_oncotated_mafs(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_match_normal_oncotated_maf(target, all_samples):\n",
    "    \"\"\"For every sample, add the path to the match normal oncotated maf file if it exists\n",
    "    Args:\n",
    "        - target: sample we are looking the match normal for\n",
    "        - all_samples: all samples to look in\n",
    "    \"\"\"\n",
    "    import pdb; pdb.set_trace()\n",
    "    if pd.isnull(target.match_normal_sample_id):\n",
    "        return target\n",
    "    match_normal = all_samples[ all_samples['entity:sample_id'] == target.match_normal_sample_id]\n",
    "    if pd.notnull(match_normal.oncotated_maf):\n",
    "        target['match_normal_oncotated_maf'] = match_normal.oncotated_maf\n",
    "    else:\n",
    "        target['match_normal_oncotated_maf'] = \"NA\"\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_samples.apply(lambda row: add_match_normal_oncotated_maf(row, remote_samples), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entity:sample_id', 'participant_id', 'clean_bam_file_capture',\n",
       "       'external_id_validation', 'aggregation_product_name_validation',\n",
       "       'bsp_sample_id_validation', 'stock_sample_id_validation', 'sample_type',\n",
       "       'picard_aggregation_type_validation', 'processed_subtype_validation',\n",
       "       'source_subtype_validation', 'squid_sample_id_validation',\n",
       "       'tumor_subtype', 'short_letter_code', 'bam_filename', 'bai_filename',\n",
       "       'clean_bai_file_capture', 'tsca_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity:sample_id</th>\n",
       "      <th>aggregation_product_name_validation</th>\n",
       "      <th>bai_filename</th>\n",
       "      <th>bam_filename</th>\n",
       "      <th>bsp_sample_id_validation</th>\n",
       "      <th>clean_bai_file_capture</th>\n",
       "      <th>clean_bam_file_capture</th>\n",
       "      <th>external_id_validation</th>\n",
       "      <th>match_normal_bam_file</th>\n",
       "      <th>match_normal_sample_id</th>\n",
       "      <th>...</th>\n",
       "      <th>__gnomad_vcf_index</th>\n",
       "      <th>__merged_vcfs_index</th>\n",
       "      <th>__mutect2_vcf_index</th>\n",
       "      <th>__scattered_intervals</th>\n",
       "      <th>__partial_mutect2_vcf</th>\n",
       "      <th>__partial_mutect2_vcf_index</th>\n",
       "      <th>filtered_variants</th>\n",
       "      <th>output_directory</th>\n",
       "      <th>clear_snvs</th>\n",
       "      <th>__vcf2_table_unfiltered_variants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>AA66-Tumor-SM-F29RQ</td>\n",
       "      <td>TSCA Rapid Cancer Detection Panel v2</td>\n",
       "      <td>2_AA66T_OPAC_p4_HKWLGBCXY.2.aligned.duplicates...</td>\n",
       "      <td>2_AA66T_OPAC_p4_HKWLGBCXY.2.aligned.duplicates...</td>\n",
       "      <td>SM-F29RQ</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>AA66T_OPAC_p4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/9...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/9...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>AB053-Tumor-SM-F29RR</td>\n",
       "      <td>TSCA Rapid Cancer Detection Panel v2</td>\n",
       "      <td>2_AB053T_OPAC_p8_2D_HKWLGBCXY.2.aligned.duplic...</td>\n",
       "      <td>2_AB053T_OPAC_p8_2D_HKWLGBCXY.2.aligned.duplic...</td>\n",
       "      <td>SM-F29RR</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>AB053T_OPAC_p8_2D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/0...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>CCLF_PEDS1023-Normal-SM-F29S6</td>\n",
       "      <td>TSCA Rapid Cancer Detection Panel v2</td>\n",
       "      <td>2_CCLF_PEDS1023N_CM_p7_HKWLGBCXY.2.aligned.dup...</td>\n",
       "      <td>2_CCLF_PEDS1023N_CM_p7_HKWLGBCXY.2.aligned.dup...</td>\n",
       "      <td>SM-F29S6</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>CCLF_PEDS1023N_CM_p7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/d...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>CCLF_PEDS1023-Tumor-SM-F29S7</td>\n",
       "      <td>TSCA Rapid Cancer Detection Panel v2</td>\n",
       "      <td>2_CCLF_PEDS1023T_RETM_p8_Hypoxia_HKWLGBCXY.2.a...</td>\n",
       "      <td>2_CCLF_PEDS1023T_RETM_p8_Hypoxia_HKWLGBCXY.2.a...</td>\n",
       "      <td>SM-F29S7</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>CCLF_PEDS1023T_RETM_p8_Hypoxia</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...</td>\n",
       "      <td>CCLF_PEDS1023-Normal-SM-F29S6</td>\n",
       "      <td>...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...</td>\n",
       "      <td>gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>[\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  entity:sample_id   aggregation_product_name_validation  \\\n",
       "32             AA66-Tumor-SM-F29RQ  TSCA Rapid Cancer Detection Panel v2   \n",
       "38            AB053-Tumor-SM-F29RR  TSCA Rapid Cancer Detection Panel v2   \n",
       "250  CCLF_PEDS1023-Normal-SM-F29S6  TSCA Rapid Cancer Detection Panel v2   \n",
       "251   CCLF_PEDS1023-Tumor-SM-F29S7  TSCA Rapid Cancer Detection Panel v2   \n",
       "\n",
       "                                          bai_filename  \\\n",
       "32   2_AA66T_OPAC_p4_HKWLGBCXY.2.aligned.duplicates...   \n",
       "38   2_AB053T_OPAC_p8_2D_HKWLGBCXY.2.aligned.duplic...   \n",
       "250  2_CCLF_PEDS1023N_CM_p7_HKWLGBCXY.2.aligned.dup...   \n",
       "251  2_CCLF_PEDS1023T_RETM_p8_Hypoxia_HKWLGBCXY.2.a...   \n",
       "\n",
       "                                          bam_filename  \\\n",
       "32   2_AA66T_OPAC_p4_HKWLGBCXY.2.aligned.duplicates...   \n",
       "38   2_AB053T_OPAC_p8_2D_HKWLGBCXY.2.aligned.duplic...   \n",
       "250  2_CCLF_PEDS1023N_CM_p7_HKWLGBCXY.2.aligned.dup...   \n",
       "251  2_CCLF_PEDS1023T_RETM_p8_Hypoxia_HKWLGBCXY.2.a...   \n",
       "\n",
       "    bsp_sample_id_validation  \\\n",
       "32                  SM-F29RQ   \n",
       "38                  SM-F29RR   \n",
       "250                 SM-F29S6   \n",
       "251                 SM-F29S7   \n",
       "\n",
       "                                clean_bai_file_capture  \\\n",
       "32   gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...   \n",
       "38   gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...   \n",
       "250  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...   \n",
       "251  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...   \n",
       "\n",
       "                                clean_bam_file_capture  \\\n",
       "32   gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...   \n",
       "38   gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...   \n",
       "250  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...   \n",
       "251  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...   \n",
       "\n",
       "             external_id_validation  \\\n",
       "32                    AA66T_OPAC_p4   \n",
       "38                AB053T_OPAC_p8_2D   \n",
       "250            CCLF_PEDS1023N_CM_p7   \n",
       "251  CCLF_PEDS1023T_RETM_p8_Hypoxia   \n",
       "\n",
       "                                 match_normal_bam_file  \\\n",
       "32                                                 NaN   \n",
       "38                                                 NaN   \n",
       "250                                                NaN   \n",
       "251  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...   \n",
       "\n",
       "            match_normal_sample_id               ...                 \\\n",
       "32                             NaN               ...                  \n",
       "38                             NaN               ...                  \n",
       "250                            NaN               ...                  \n",
       "251  CCLF_PEDS1023-Normal-SM-F29S6               ...                  \n",
       "\n",
       "                                    __gnomad_vcf_index  \\\n",
       "32   gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/9...   \n",
       "38                                                 NaN   \n",
       "250  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...   \n",
       "251  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...   \n",
       "\n",
       "                                   __merged_vcfs_index  \\\n",
       "32   gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/9...   \n",
       "38                                                 NaN   \n",
       "250  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...   \n",
       "251  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...   \n",
       "\n",
       "                                   __mutect2_vcf_index  \\\n",
       "32   [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "38   gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/0...   \n",
       "250  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...   \n",
       "251  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/8...   \n",
       "\n",
       "                                 __scattered_intervals  \\\n",
       "32   [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "38   [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "250  [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "251  [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "\n",
       "                                 __partial_mutect2_vcf  \\\n",
       "32                                                 NaN   \n",
       "38   [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "250  [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "251  [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "\n",
       "                           __partial_mutect2_vcf_index  \\\n",
       "32                                                 NaN   \n",
       "38   [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "250  [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "251  [\"gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff...   \n",
       "\n",
       "                                     filtered_variants  \\\n",
       "32                                                 NaN   \n",
       "38                                                 NaN   \n",
       "250  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/d...   \n",
       "251                                               null   \n",
       "\n",
       "                                      output_directory clear_snvs  \\\n",
       "32                                                 NaN        NaN   \n",
       "38                                                 NaN        NaN   \n",
       "250  gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/s...        NaN   \n",
       "251                                                NaN       null   \n",
       "\n",
       "    __vcf2_table_unfiltered_variants  \n",
       "32                               NaN  \n",
       "38                               NaN  \n",
       "250                              NaN  \n",
       "251                             null  \n",
       "\n",
       "[4 rows x 59 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_samples[ pd.notnull(remote_samples.oncotated_maf) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_samples = compile_samples(path_to_all_samples_info, google_bucket_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entity:sample_id', 'aggregation_product_name_validation',\n",
       "       'bai_filename', 'bam_filename', 'bsp_sample_id_validation',\n",
       "       'clean_bai_file_capture', 'clean_bam_file_capture',\n",
       "       'external_id_validation', 'match_normal_bam_file',\n",
       "       'match_normal_sample_id', 'match_primary_tumor_bam_file',\n",
       "       'match_primary_tumor_sample_id', 'participant',\n",
       "       'picard_aggregation_type_validation', 'processed_subtype_validation',\n",
       "       'renamed_bam_file', 'sample_type', 'short_letter_code',\n",
       "       'source_subtype_validation', 'squid_sample_id_validation',\n",
       "       'stock_sample_id_validation', 'target_coverage', 'tsca_id',\n",
       "       'tumor_subtype', 'cnv_calls', 'tumor_ptn', 'tumor_seg', 'tumor_tn',\n",
       "       'annotate_variants_vcf', 'annotate_variants_vcf_index', 'mutect2_vcf',\n",
       "       'mutect2_vcf_index', 'sample_cum_cov', 'sample_cum_cov_prop',\n",
       "       'sample_gene_summary', 'sample_interval_statistics',\n",
       "       'sample_interval_summary', 'sample_statistics', 'sample_summary',\n",
       "       'scattered_intervals', 'cosmic_vcf', 'exac_vcf', 'gnomad_vcf',\n",
       "       'merged_vcfs', 'oncotated_maf', 'oncotated_vcf',\n",
       "       '__annotate_variants_vcf_index', '__cosmic_vcf_index',\n",
       "       '__exac_vcf_index', '__gnomad_vcf_index', '__merged_vcfs_index',\n",
       "       '__mutect2_vcf_index', '__scattered_intervals', '__partial_mutect2_vcf',\n",
       "       '__partial_mutect2_vcf_index', 'filtered_variants', 'output_directory',\n",
       "       'clear_snvs', '__vcf2_table_unfiltered_variants'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entity:sample_id', 'participant_id', 'clean_bam_file_capture',\n",
       "       'external_id_validation', 'aggregation_product_name_validation',\n",
       "       'bsp_sample_id_validation', 'stock_sample_id_validation', 'sample_type',\n",
       "       'picard_aggregation_type_validation', 'processed_subtype_validation',\n",
       "       'source_subtype_validation', 'squid_sample_id_validation',\n",
       "       'tumor_subtype', 'short_letter_code', 'bam_filename', 'bai_filename',\n",
       "       'clean_bai_file_capture', 'tsca_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "udpated_local_samples = pd.merge(local_samples, remote_samples[['entity:sample_id', 'oncotated_maf']], on='entity:sample_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "udpated_local_samples.to_csv(\"match_normal_updates/updated_sample_data.tsv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = upload_entities_from_tsv(namespace, workspace, \"match_normal_updates/updated_sample_data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
